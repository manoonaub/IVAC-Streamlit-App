# sections/intro.py
import streamlit as st
import pandas as pd
import plotly.express as px

from utils.io import load_data
from utils.prep import clean_ivac, diff_columns_breakdown

LANG_TEXT = {
    # EN
"metric_delta_fmt": "{eng} added / {drop} dropped (net {net:+d})",

# FR
"metric_delta_fmt": "{eng} ajoutÃ©es / {drop} supprimÃ©es (net {net:+d})",
    "en": {
        "title": "ðŸŽ“ IVAC â€“ Measuring the True Impact of French Middle Schools",
        "hook1": (
            "Every year, more than **800,000 students** take the DiplÃ´me National du Brevet (DNB). "
            "But beyond pass rates, **how can we know which schools truly help students progress?**"
        ),
        "hook2": (
            "The **Added Value Indicators of Middle Schools (IVAC)** published by the French Ministry of National Education "
            "aim to answer this question. They estimate how much a school *contributes* to student success after accounting "
            "for **prior level, age, and socio-economic background**."
        ),
        "def_va": (
            "> A high *added value* means a school helps its students perform **better than expected** compared to similar schools. "
            "A negative value may indicate challenges that limit student progress."
        ),
        "context_title": "ðŸŽ¯ Project Context & Research Question",
        "rq": "Which middle schools in France demonstrate the strongest **added value**, and what patterns explain these differences?",
        "goals": (
            "- Analyze **trends and inequalities** across regions and sessions (2022â€“2024)\n"
            "- Compare **public vs. private** institutions in terms of added value\n"
            "- Identify **top/bottom schools** to understand what drives success"
        ),
        "source": "Source: French Ministry of National Education â€” data.gouv.fr â€” Etalab 2.0 license",
        "prep_title": "ðŸ§¹ Data Preparation Summary",
        "applied_steps": (
            "**Applied steps**:\n"
            "- Harmonization of column names and types\n"
            "- Standardization of numeric and categorical fields\n"
            "- Creation of derived indicators such as `valeur_ajoutee` (added value)\n"
            "- Aggregations by **academic region** and **exam session**\n\n"
            "These transformations ensure the data is clean, comparable, and ready for interactive analysis."
        ),
        "preview_title": "ðŸ“‹ Preview of the first 5 rows (standardized)",
        "checklist_title": "âœ… Dataset Selection & Relevance Checklist",
        "checklist_table": (
            "| Criterion | Justification |\n"
            "|:--|:--|\n"
            "| **Relevance** | Education & school performance; linked to public policy. |\n"
            "| **Granularity** | By session (time) and academy/department (geography). |\n"
            "| **Quality & Metadata** | Official open dataset with documentation; Etalab 2.0 license. |\n"
            "| **Audience** | Educators, policymakers, researchers, parents. |\n"
            "| **Size & Usability** | ~20,000 rows; manageable locally for interactive analysis. |"
        ),
        "narrative_title": "ðŸ§­ Narrative Design",
        "narrative": (
            "We follow a **Comparative & Ranking** pattern:\n"
            "1. **Data Quality & Profiling** â†’ assess reliability and completeness\n"
            "2. **Visualization & Analysis** â†’ compare regions/sectors and highlight inequalities\n"
            "3. **Deep Dives** â†’ detect outliers and top/bottom schools\n"
            "4. **Conclusions** â†’ insights, implications, recommendations"
        ),
        "next": "ðŸ§© **Next step:** go to **Data Quality & Profiling** to validate key indicators before analysis.",
        "metric_rows": "Rows",
        "metric_cols": "Columns",
        "metric_dups": "Duplicate rows",
        "metric_academies": "Unique academies",
        "metric_schools": "Unique schools",
        "language": "Language",
        "en": "English",
        "fr": "FranÃ§ais",
        "std_note": "ðŸ§© *Standardized* means the raw IVAC data has been cleaned and harmonized (column names, types, missing values) so it is comparable across schools and regions.",
        # donut / details
        "donut_title": "Column structure (before/after cleaning)",
        "donut_kept": "Kept/Renamed",
        "donut_added": "Added (engineered)",
        "donut_dropped": "Dropped",
        "eng_cols_caption": "ðŸ§© **Engineered columns added:**",
        "renamed_expander": "ðŸ”„ Show renamed columns (raw â†’ cleaned)",
        "no_renamed": "No renamed columns detected.",
        "dropped_caption": "ðŸ—‘ï¸ Dropped columns:",
    },
    "fr": {
        "title": "ðŸŽ“ IVAC â€“ Mesurer lâ€™impact rÃ©el des collÃ¨ges en France",
        "hook1": (
            "Chaque annÃ©e, plus de **800 000 Ã©lÃ¨ves** passent le DiplÃ´me National du Brevet (DNB). "
            "Mais au-delÃ  du taux de rÃ©ussite, **comment savoir quels collÃ¨ges font rÃ©ellement progresser leurs Ã©lÃ¨ves ?**"
        ),
        "hook2": (
            "Les **Indicateurs de Valeur AjoutÃ©e des CollÃ¨ges (IVAC)**, publiÃ©s par le MinistÃ¨re de lâ€™Ã‰ducation nationale, "
            "cherchent Ã  rÃ©pondre Ã  cette question. Ils estiment la part de rÃ©ussite *attribuable au collÃ¨ge* en neutralisant "
            "le **niveau antÃ©rieur, lâ€™Ã¢ge et le contexte socio-Ã©conomique** des Ã©lÃ¨ves."
        ),
        "def_va": (
            "> Une *valeur ajoutÃ©e* Ã©levÃ©e signifie quâ€™un collÃ¨ge fait rÃ©ussir ses Ã©lÃ¨ves **mieux quâ€™attendu** "
            "par rapport Ã  des Ã©tablissements comparables. Une valeur nÃ©gative peut rÃ©vÃ©ler des difficultÃ©s structurelles."
        ),
        "context_title": "ðŸŽ¯ Contexte & Question de recherche",
        "rq": "Quels collÃ¨ges en France prÃ©sentent la plus forte **valeur ajoutÃ©e**, et quels schÃ©mas expliquent ces Ã©carts ?",
        "goals": (
            "- Analyser les **tendances et inÃ©galitÃ©s** entre acadÃ©mies et sessions (2022â€“2024)\n"
            "- Comparer **secteur public vs privÃ©** en termes de valeur ajoutÃ©e\n"
            "- Identifier les **meilleurs/moins bons collÃ¨ges** pour comprendre les facteurs de succÃ¨s"
        ),
        "source": "Source : MinistÃ¨re de lâ€™Ã‰ducation nationale â€” data.gouv.fr â€” Licence Etalab 2.0",
        "prep_title": "ðŸ§¹ SynthÃ¨se de la prÃ©paration des donnÃ©es",
        "applied_steps": (
            "**Ã‰tapes appliquÃ©es** :\n"
            "- Harmonisation des noms de colonnes et des types\n"
            "- Standardisation des variables numÃ©riques et catÃ©gorielles\n"
            "- CrÃ©ation dâ€™indicateurs dÃ©rivÃ©s comme `valeur_ajoutee`\n"
            "- AgrÃ©gations par **rÃ©gion acadÃ©mique** et **session**\n\n"
            "Ces transformations garantissent des comparaisons fiables et reproductibles."
        ),
        "preview_title": "ðŸ“‹ AperÃ§u des 5 premiÃ¨res lignes (standardisÃ©es)",
        "checklist_title": "âœ… Checklist â€” Choix du jeu de donnÃ©es",
        "checklist_table": (
            "| CritÃ¨re | Justification |\n"
            "|:--|:--|\n"
            "| **Pertinence** | Ã‰ducation & performance scolaire ; utile pour lâ€™action publique. |\n"
            "| **GranularitÃ©** | Par session (temps) et acadÃ©mie/dÃ©partement (gÃ©o). |\n"
            "| **QualitÃ© & MÃ©tadonnÃ©es** | Open data officielle documentÃ©e ; licence Etalab 2.0. |\n"
            "| **Public** | Ã‰quipe Ã©ducative, dÃ©cideurs, chercheurs, parents. |\n"
            "| **Taille & usage** | ~20 000 lignes ; exploitable localement en mode interactif. |"
        ),
        "narrative_title": "ðŸ§­ Trame narrative",
        "narrative": (
            "Nous suivons un schÃ©ma **Comparaisons & Classements** :\n"
            "1. **Data Quality & Profiling** â†’ fiabilitÃ© et complÃ©tude\n"
            "2. **Visualization & Analysis** â†’ comparaisons rÃ©gions/secteurs et inÃ©galitÃ©s\n"
            "3. **Deep Dives** â†’ dÃ©tection dâ€™outliers et top/bottom\n"
            "4. **Conclusions** â†’ enseignements, implications, recommandations"
        ),
        "next": "ðŸ§© **Ã‰tape suivante :** ouvrir **Data Quality & Profiling** pour valider les indicateurs clÃ©s avant lâ€™analyse.",
        "metric_rows": "Lignes",
        "metric_cols": "Colonnes",
        "metric_dups": "Doublons",
        "metric_academies": "AcadÃ©mies uniques",
        "metric_schools": "CollÃ¨ges uniques",
        "language": "Langue",
        "en": "English",
        "fr": "FranÃ§ais",
        "std_note": "ðŸ§© *StandardisÃ©* signifie que les donnÃ©es IVAC brutes ont Ã©tÃ© nettoyÃ©es et harmonisÃ©es (noms de colonnes, types, valeurs manquantes) pour Ãªtre comparables entre collÃ¨ges et rÃ©gions.",
        # donut / dÃ©tails
        "donut_title": "Structure des colonnes (avant/aprÃ¨s nettoyage)",
        "donut_kept": "ConservÃ©es/RenommÃ©es",
        "donut_added": "AjoutÃ©es (engineered)",
        "donut_dropped": "SupprimÃ©es",
        "eng_cols_caption": "ðŸ§© **Colonnes dÃ©rivÃ©es ajoutÃ©es :**",
        "renamed_expander": "ðŸ”„ Voir les colonnes renommÃ©es (raw â†’ cleaned)",
        "no_renamed": "Aucune colonne renommÃ©e dÃ©tectÃ©e.",
        "dropped_caption": "ðŸ—‘ï¸ Colonnes supprimÃ©es :",
    },
}

def _get_lang() -> str:
    lang = st.query_params.get("lang", "en")
    return lang if lang in ("en", "fr") else "en"

def _set_lang(lang: str):
    st.query_params["lang"] = lang
def show():
    # ---- Language switcher (sidebar) ----
    current_lang = _get_lang()
    st.sidebar.subheader(LANG_TEXT[current_lang].get("language", "Language"))
    choice = st.sidebar.radio(
        label=" ",
        options=["en", "fr"],
        format_func=lambda x: LANG_TEXT[x].get(x, x.title()),
        horizontal=True,
        key="lang_selector_intro",
        label_visibility="collapsed",
    )
    if choice and choice != current_lang:
        _set_lang(choice)
        st.rerun()
    T = LANG_TEXT.get(choice or current_lang, LANG_TEXT["en"])

    # ---------------- TITLE & HOOK ----------------
    st.title(T.get("title", "IVAC â€“ Added Value Indicators"))
    st.markdown(f"{T.get('hook1','')}\n\n{T.get('hook2','')}")
    if T.get("def_va"):
        st.info(T["def_va"])

    # ---------------- CONTEXT & QUESTION ----------------
    if T.get("context_title"):
        st.subheader(T["context_title"])
    rq = T.get("rq", "")
    goals = T.get("goals", "")
    if rq or goals:
        st.markdown(f"> **{rq}**\n\n{goals}")
    if T.get("source"):
        st.caption(T["source"])

    # ---------------- CLEANING SUMMARY ----------------
    if T.get("prep_title"):
        st.subheader(T["prep_title"])

    # Load / clean / diff
    df_raw = load_data()
    df_clean = clean_ivac(df_raw)
    diff = diff_columns_breakdown(df_raw, df_clean)

    # ---- KPIs
    c1, c2, c3, c4, c5 = st.columns(5)
    c1.metric(T.get("metric_rows", "Rows"), f"{df_raw.shape[0]:,}".replace(",", " "))

    engineered_count = len(diff.get("engineered", []))
    dropped_count    = len(diff.get("dropped", []))
    net_delta        = engineered_count - dropped_count

    delta_fmt = T.get(
        "metric_delta_fmt",
        "{eng} added / {drop} dropped (net {net:+d})"
    )
    c2.metric(
        f"{T.get('metric_cols','Columns')} (raw â†’ cleaned)",
        f"{df_raw.shape[1]} â†’ {df_clean.shape[1]}",
        delta=delta_fmt.format(eng=engineered_count, drop=dropped_count, net=net_delta),
    )

    c3.metric(T.get("metric_dups", "Duplicates"), str(df_raw.duplicated().sum()))

    n_academies = df_clean["region_academique"].nunique() if "region_academique" in df_clean.columns else "N/A"
    n_schools   = df_clean["uai"].nunique() if "uai" in df_clean.columns else "N/A"
    c4.metric(T.get("metric_academies", "Academic regions"), n_academies)
    c5.metric(T.get("metric_schools", "Unique schools (UAI)"), n_schools)
    st.caption("Counts computed on the cleaned dataset (all sessions combined).")
    # ---- Donut: column structure
    raw_cols_count = df_raw.shape[1]
    kept_or_renamed_count = max(raw_cols_count - dropped_count, 0)

    donut_df = pd.DataFrame({
        "Category": [
            T.get("donut_kept", "Kept/Renamed"),
            T.get("donut_added", "Added (engineered)"),
            T.get("donut_dropped", "Dropped"),
        ],
        "Count": [kept_or_renamed_count, engineered_count, dropped_count],
    })
    fig = px.pie(
        donut_df, values="Count", names="Category",
        title=T.get("donut_title", "Column structure after cleaning"),
        hole=0.4
    )
    fig.update_traces(textposition="inside", textinfo="percent+label")
    st.plotly_chart(fig, use_container_width=True)
    st.markdown(
        "> **Legend** â€” *Kept/Renamed*: original columns kept or renamed Â· "
        "*Added (engineered)*: columns created during cleaning (e.g., `valeur_ajoutee`, `row_id`) Â· "
        "*Dropped*: columns removed (duplicates, replaced, or irrelevant)."
    )
    assert df_clean.shape[0] == df_raw.shape[0]  # on ne perd pas de lignes au cleaning
    for col in ["valeur_ajoutee", "nb_candidats_total", "row_id"]:
      assert col in df_clean.columns
    # ---- Details
    if diff.get("engineered"):
        st.caption(f"{T.get('eng_cols_caption','Engineered columns added:')} {', '.join(diff['engineered'])}")

    with st.expander(T.get("renamed_expander", "Show renamed columns (raw â†’ cleaned)")):
        renamed_data = diff.get("renamed", [])
        if renamed_data:
            st.dataframe(
                pd.DataFrame(renamed_data, columns=["Raw Name", "Clean Name"]),
                use_container_width=True,
            )
        else:
            st.write(T.get("no_renamed", "No renamed columns detected."))

    if diff.get("dropped"):
        st.caption(f"{T.get('dropped_caption','Dropped columns:')} {', '.join(diff['dropped'])}")

    # ---- Steps text
    if T.get("applied_steps"):
        st.markdown(T["applied_steps"])

    # ---- Preview + note
    if T.get("preview_title"):
        st.subheader(T["preview_title"])
    st.dataframe(df_clean.head(), use_container_width=True)
    if T.get("std_note"):
        st.caption(T["std_note"])

    # ---- Checklist & narrative
    if T.get("checklist_title"):
        st.subheader(T["checklist_title"])
    if T.get("checklist_table"):
        st.markdown(T["checklist_table"])

    if T.get("narrative_title"):
        st.subheader(T["narrative_title"])
    if T.get("narrative"):
        st.markdown(T["narrative"])

    # ---- Next
    if T.get("next"):
        st.info(T["next"])